{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "# Neural Network \n",
    "from sklearn.feature_selection import RFECV    # for feature selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train.csv\")\n",
    "df.columns = list(df.columns.values)\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "df_test.columns = list(df_test.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Additional Variables\n",
    "Result: These indicator variables definitely worsen the performance of Logistic Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD (bid volumes / ask volumes) ratio  TO TRAIN AND TEST \n",
    "# indicator - ratio of all bid volumes to all ask volumes\n",
    "df_test = df_test.assign(bid_ask_ratio_all = \\\n",
    "                        lambda x: (x.bid1vol+x.bid2vol+x.bid3vol+x.bid4vol+x.bid5vol) / \\\n",
    "                                  (x.ask1vol+x.ask2vol+x.ask3vol+x.ask4vol+x.ask5vol))\n",
    "\n",
    "# indicator - ratio of all bid volumes to all ask volumes\n",
    "df = df.assign(bid_ask_ratio_all = \\\n",
    "                        lambda x: (x.bid1vol+x.bid2vol+x.bid3vol+x.bid4vol+x.bid5vol) / \\\n",
    "                                  (x.ask1vol+x.ask2vol+x.ask3vol+x.ask4vol+x.ask5vol))\n",
    "\n",
    "\n",
    "# Add (best bid volume / best ask volume) TO TRAIN AND TEST \n",
    "# indicator - ratio of best bid (bid1) volume to best ask (ask1) volume\n",
    "\n",
    "#df_test = df_test.assign(bid_ask_ratio_best = lambda x: x.bid1vol / x.ask1vol)\n",
    "#df = df.assign(bid_ask_ratio_best = lambda x: x.bid1vol / x.ask1vol)\n",
    "\n",
    "\n",
    "# indicator - difference between the price of most recent order and average of best bid and best ask (mid)\n",
    "#df_test = df_test.assign(diff_price = lambda x: (x.last_price) - (x.mid))\n",
    "#df = df.assign(diff_price = lambda x: (x.last_price) - (x.mid))\n",
    "\n",
    "# indicator - difference between best bid price and best ask price\n",
    "#df_test = df_test.assign(diff_best_bid_ask = lambda x: (x.bid1) - (x.ask1))\n",
    "#df = df.assign(diff_best_bid_ask = lambda x: (x.bid1) - (x.ask1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop [bid2, ... bid5, ask2, ... ask5] columns due to heavy correlation \n",
    "df.drop(columns=['bid1vol','bid2vol', 'bid3vol', 'bid4vol', 'bid5vol', 'ask1vol','ask2vol',\\\n",
    "                 'ask3vol', 'ask4vol', 'ask5vol'], inplace=True)\n",
    "df_test.drop(columns=['bid1vol','bid2vol', 'bid3vol', 'bid4vol', 'bid5vol', 'ask1vol','ask2vol',\\\n",
    "                 'ask3vol', 'ask4vol', 'ask5vol'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping Highly Correlated Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.drop(columns=['closed_position_qty', 'opened_position_qty '], inplace=True)\n",
    "df_test.drop(columns=['closed_position_qty', 'opened_position_qty '], inplace=True)\n",
    "\n",
    "# Let's drop [bid2, ... bid5, ask2, ... ask5] columns due to heavy correlation \n",
    "df.drop(columns=['bid2', 'bid3', 'bid4', 'bid5', 'ask2', 'ask3', \\\n",
    "                 'ask4', 'ask5'], inplace=True)\n",
    "df_test.drop(columns=['bid2', 'bid3', 'bid4', 'bid5', 'ask2', 'ask3', \\\n",
    "                 'ask4', 'ask5'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>last_price</th>\n",
       "      <th>mid</th>\n",
       "      <th>transacted_qty</th>\n",
       "      <th>d_open_interest</th>\n",
       "      <th>bid1</th>\n",
       "      <th>ask1</th>\n",
       "      <th>bid1vol</th>\n",
       "      <th>bid2vol</th>\n",
       "      <th>bid3vol</th>\n",
       "      <th>bid4vol</th>\n",
       "      <th>bid5vol</th>\n",
       "      <th>ask1vol</th>\n",
       "      <th>ask2vol</th>\n",
       "      <th>ask3vol</th>\n",
       "      <th>ask4vol</th>\n",
       "      <th>ask5vol</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3842.4</td>\n",
       "      <td>3842.6</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3842.4</td>\n",
       "      <td>3842.8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3842.8</td>\n",
       "      <td>3843.4</td>\n",
       "      <td>55.0</td>\n",
       "      <td>-43</td>\n",
       "      <td>3843.0</td>\n",
       "      <td>3843.8</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3844.0</td>\n",
       "      <td>3844.3</td>\n",
       "      <td>84.0</td>\n",
       "      <td>-69</td>\n",
       "      <td>3843.8</td>\n",
       "      <td>3844.8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3843.8</td>\n",
       "      <td>3843.4</td>\n",
       "      <td>37.0</td>\n",
       "      <td>-30</td>\n",
       "      <td>3843.0</td>\n",
       "      <td>3843.8</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3843.2</td>\n",
       "      <td>3843.1</td>\n",
       "      <td>41.0</td>\n",
       "      <td>-35</td>\n",
       "      <td>3842.8</td>\n",
       "      <td>3843.4</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  last_price     mid  transacted_qty  d_open_interest    bid1    ask1  \\\n",
       "0   0      3842.4  3842.6           103.0                0  3842.4  3842.8   \n",
       "1   1      3842.8  3843.4            55.0              -43  3843.0  3843.8   \n",
       "2   2      3844.0  3844.3            84.0              -69  3843.8  3844.8   \n",
       "3   3      3843.8  3843.4            37.0              -30  3843.0  3843.8   \n",
       "4   4      3843.2  3843.1            41.0              -35  3842.8  3843.4   \n",
       "\n",
       "   bid1vol  bid2vol  bid3vol  bid4vol  bid5vol  ask1vol  ask2vol  ask3vol  \\\n",
       "0        8        1        6       14        6        6        1        1   \n",
       "1        7        6       11        1        6        1        4        4   \n",
       "2        3        1        4       21       12        1       16       10   \n",
       "3       10       13       12        2        4        2        7        1   \n",
       "4       14       12        2        2        4        1        3        1   \n",
       "\n",
       "   ask4vol  ask5vol  y  \n",
       "0       10        2  1  \n",
       "1        1       13  0  \n",
       "2        4        9  0  \n",
       "3        2       11  1  \n",
       "4       11       15  1  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate data into X and y, Normalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating into X and y training \n",
    "last_idx = len(df.columns) - 1\n",
    "X = df[df.columns[:-1]]\n",
    "#y_train = df[df.columns[last_idx]]\n",
    "y_train = df.y\n",
    "X = np.array(X)\n",
    "y_train = np.array(y_train)\n",
    "X_test = df_test\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# HANDLING IDs FOR TRAINING \n",
    "X_ids = X[:, 0]  # Vector of id's \n",
    "X = X[:, 1:]\n",
    "# HANDLING IDs for TEST\n",
    "X_test_ids = X_test[:, 0]\n",
    "X_test = X_test[:, 1:]\n",
    "\n",
    "# Normalize test set features using training set stats\n",
    "for j in range(0,len(X_test[0])):\n",
    "    X_std = np.std(X[:, j])\n",
    "    X_test[:, j] -= np.mean(X[:, j])\n",
    "    X_test[:, j] *= (1 / X_std)\n",
    "\n",
    "# Normalize training set features \n",
    "for j in range(0,len(X[0])):\n",
    "    X_std = np.std(X[:, j])\n",
    "    X[:, j] -= np.mean(X[:, j])\n",
    "    X[:, j] *= (1 / X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Reg (no regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try logistic regression for different lambda = 0.00001,\n",
    "# 0.00005, 0.00025, 61, 035.15625\n",
    "lam = 0.0005 # Penalizing to take care of some strong correlation between features  \n",
    "# C is inverse of regularization strength \n",
    "\n",
    "#base = np.repeat(5, 5)\n",
    "#power = np.arange(0, 5)\n",
    "#lambdas = np.power(base, power)\n",
    "#final_lambdas = 0.00001 * lambdas\n",
    "\n",
    "# The C's (inv regularization strength) range from 1e-5 ... 1e5\n",
    "clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y_train)\n",
    "#clf = LogisticRegression(penalty='l2', C=(1/lam)).fit(X, y_trai )\n",
    "y_pred = clf.predict_proba(X)\n",
    "y_pred = y_pred[:,1]    # Grabbing probabililities of being in class 1\n",
    "\n",
    "\n",
    "# Predicting on test set \n",
    "y_pred_test = clf.predict_proba(X_test)\n",
    "y_pred_test = y_pred_test[:,1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test AUC-ROC score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K-fold cross-validation results: \n",
      "LogisticRegressionCV average accuracy: 0.651 (+/-0.001)\n",
      "LogisticRegressionCV average log_loss: 0.628 (+/-0.002)\n",
      "LogisticRegressionCV average auc: 0.639 (+/-0.006)\n"
     ]
    }
   ],
   "source": [
    "scoring = {'accuracy': 'accuracy', 'log_loss': 'neg_log_loss', 'auc': 'roc_auc'}\n",
    "\n",
    "#clf_test = RandomForestClassifier(max_depth=5, min_samples_split=2, \n",
    "                                 # random_state=0, n_estimators=10, \n",
    "                                 # max_features='sqrt', criterion='gini')\n",
    "\n",
    "results = cross_validate(clf, X, y_train, cv=5, scoring=list(scoring.values()), \n",
    "                         return_train_score=False)\n",
    "    \n",
    "print('\\nK-fold cross-validation results: ')\n",
    "for sc in range(len(scoring)):\n",
    "    print(clf.__class__.__name__+\" average %s: %.3f (+/-%.3f)\" % (list(scoring.keys())[sc], -results['test_%s' % list(scoring.values())[sc]].mean()\n",
    "    if list(scoring.values())[sc]=='neg_log_loss' \n",
    "    else results['test_%s' % list(scoring.values())[sc]].mean(), \n",
    "        results['test_%s' % list(scoring.values())[sc]].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best logistic regression model: no regularization, all columns except closed_position_qty\n",
    "and opened_position_qty, [ask2, ... ask5], [bid2, ... bid5] with 5-fold stratified \n",
    "cross-validation achieves a score of 0.60549. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Dataset for ANN training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train.csv\")\n",
    "df.columns = list(df.columns.values)\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "df_test.columns = list(df_test.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Betty's preprocessing code and final features (from a day ago)\n",
    "# Filling NaN values \n",
    "nan_values = {'opened_position_qty ': (df['transacted_qty'] + df['d_open_interest']) / 2 , \n",
    "              'closed_position_qty': (df['transacted_qty'] - df['d_open_interest']) / 2}\n",
    "\n",
    "nan_values_test = {'opened_position_qty ': (df_test['transacted_qty'] + df_test['d_open_interest']) / 2 , \n",
    "              'closed_position_qty': (df_test['transacted_qty'] - df_test['d_open_interest']) / 2}\n",
    "\n",
    "df = df.fillna(value=nan_values)\n",
    "df_test = df_test.fillna(value=nan_values_test)\n",
    "\n",
    "\n",
    "\n",
    "# ADD (bid volumes / ask volumes) ratio  TO TRAIN AND TEST \n",
    "# indicator - ratio of all bid volumes to all ask volumes\n",
    "df_test = df_test.assign(bid_ask_ratio_all = \\\n",
    "                        lambda x: (x.bid1vol+x.bid2vol+x.bid3vol+x.bid4vol+x.bid5vol) / \\\n",
    "                                  (x.ask1vol+x.ask2vol+x.ask3vol+x.ask4vol+x.ask5vol))\n",
    "\n",
    "# indicator - ratio of all bid volumes to all ask volumes\n",
    "df = df.assign(bid_ask_ratio_all = \\\n",
    "                        lambda x: (x.bid1vol+x.bid2vol+x.bid3vol+x.bid4vol+x.bid5vol) / \\\n",
    "                                  (x.ask1vol+x.ask2vol+x.ask3vol+x.ask4vol+x.ask5vol))\n",
    "\n",
    "\n",
    "# Add (best bid volume / best ask volume) TO TRAIN AND TEST \n",
    "# indicator - ratio of best bid (bid1) volume to best ask (ask1) volume\n",
    "\n",
    "df_test = df_test.assign(bid_ask_ratio_best = lambda x: x.bid1vol / x.ask1vol)\n",
    "df = df.assign(bid_ask_ratio_best = lambda x: x.bid1vol / x.ask1vol)\n",
    "\n",
    "\n",
    "# indicator - difference between the price of most recent order and average of best bid and best ask (mid)\n",
    "df_test = df_test.assign(diff_price = lambda x: (x.last_price) - (x.mid))\n",
    "df = df.assign(diff_price = lambda x: (x.last_price) - (x.mid))\n",
    "\n",
    "# indicator - difference between best bid price and best ask price\n",
    "df_test = df_test.assign(diff_best_bid_ask = lambda x: (x.bid1) - (x.ask1))\n",
    "df = df.assign(diff_best_bid_ask = lambda x: (x.bid1) - (x.ask1))\n",
    "\n",
    "cols = ['last_price', \n",
    "        'transacted_qty', 'd_open_interest', \n",
    "        'bid_ask_ratio_all', 'bid_ask_ratio_best', \n",
    "        'diff_price', 'diff_best_bid_ask']\n",
    "\n",
    "X = np.asarray(df[cols])\n",
    "y_train = np.asarray(df['y'])\n",
    "\n",
    "X_test = np.asarray(df_test[cols])  # grabbing test set \n",
    "\n",
    "\n",
    "# Normalize test set features using training set stats\n",
    "for j in range(0,len(X_test[0])):\n",
    "    X_std = np.std(X[:, j])\n",
    "    X_test[:, j] -= np.mean(X[:, j])\n",
    "    X_test[:, j] *= (1 / X_std)\n",
    "\n",
    "# Normalize training set features \n",
    "for j in range(0,len(X[0])):\n",
    "    X_std = np.std(X[:, j])\n",
    "    X[:, j] -= np.mean(X[:, j])\n",
    "    X[:, j] *= (1 / X_std)\n",
    "\n",
    "# Adding indicators \n",
    "# indicator - ratio of all bid volumes to all ask volumes\n",
    "#df_ex = df.assign(bid_ask_ratio_all = \\\n",
    "                     #   lambda x: (x.bid1vol+x.bid2vol+x.bid3vol+x.bid4vol+x.bid5vol) / \\\n",
    "                       #           (x.ask1vol+x.ask2vol+x.ask3vol+x.ask4vol+x.ask5vol))\n",
    "# indicator - ratio of best bid (bid1) volume to best ask (bid2) volume\n",
    "#df_ex = df_ex.assign(bid_ask_ratio_best = lambda x: x.bid1vol / x.ask1vol)\n",
    "\n",
    "# indicator - difference between the price of most recent order and average of best bid and best ask (mid)\n",
    "#df_ex = df_ex.assign(diff_price = lambda x: (x.last_price) - (x.mid))\n",
    "\n",
    "# indicator - difference between best bid price and best ask price\n",
    "#df_ex = df_ex.assign(diff_best_bid_ask = lambda x: (x.bid1) - (x.ask1))\n",
    "#cols = ['last_price', \n",
    "    #    'transacted_qty', 'd_open_interest', \n",
    "     #   'bid_ask_ratio_all', 'bid_ask_ratio_best', \n",
    "      #  'diff_price', 'diff_best_bid_ask']\n",
    "\n",
    "#X = np.asarray(df_ex[cols])\n",
    "#y_train = np.asarray(df_ex['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(5,input_dim=7, activation='relu'))# Input layer, size (,7)\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output probability of midprice \n",
    "                                           # going up. \n",
    "\n",
    "    # Maybe keep more features??? Because this is supposed to be \n",
    "    # a non-linear model, so correlation shouldn't matter too much. \n",
    "    \n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', \\\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "592380/592380 [==============================] - 7s 11us/step - loss: 0.6255 - acc: 0.6528\n",
      "Epoch 2/15\n",
      "592380/592380 [==============================] - 7s 12us/step - loss: 0.6189 - acc: 0.6578\n",
      "Epoch 3/15\n",
      "592380/592380 [==============================] - 7s 12us/step - loss: 0.6184 - acc: 0.6583\n",
      "Epoch 4/15\n",
      "592380/592380 [==============================] - 7s 13us/step - loss: 0.6181 - acc: 0.6581\n",
      "Epoch 5/15\n",
      "592380/592380 [==============================] - 8s 13us/step - loss: 0.6180 - acc: 0.6586\n",
      "Epoch 6/15\n",
      "592380/592380 [==============================] - 9s 15us/step - loss: 0.6180 - acc: 0.6583\n",
      "Epoch 7/15\n",
      "592380/592380 [==============================] - 9s 14us/step - loss: 0.6179 - acc: 0.6586\n",
      "Epoch 8/15\n",
      "592380/592380 [==============================] - 8s 13us/step - loss: 0.6179 - acc: 0.6586\n",
      "Epoch 9/15\n",
      "592380/592380 [==============================] - 8s 14us/step - loss: 0.6179 - acc: 0.6584:\n",
      "Epoch 10/15\n",
      "592380/592380 [==============================] - 8s 14us/step - loss: 0.6179 - acc: 0.6588\n",
      "Epoch 11/15\n",
      "592380/592380 [==============================] - 8s 13us/step - loss: 0.6179 - acc: 0.6586\n",
      "Epoch 12/15\n",
      "592380/592380 [==============================] - 8s 13us/step - loss: 0.6178 - acc: 0.6587\n",
      "Epoch 13/15\n",
      "592380/592380 [==============================] - 8s 14us/step - loss: 0.6177 - acc: 0.6590\n",
      "Epoch 14/15\n",
      "592380/592380 [==============================] - 8s 14us/step - loss: 0.6176 - acc: 0.6588\n",
      "Epoch 15/15\n",
      "592380/592380 [==============================] - 8s 13us/step - loss: 0.6175 - acc: 0.6589\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, y_train, epochs=15, batch_size=64)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputting Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6558090619590976"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:10]\n",
    "roc_auc_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix output format\n",
    "X_test_ids = list(df_test['id'].astype(int))\n",
    "#X_test_ids = df['id'].astype(int)\n",
    "y_pred_flat = np.ndarray.flatten(y_pred)\n",
    "predictions = {'id': X_test_ids, 'Predicted': y_pred_flat}\n",
    "df_output = pd.DataFrame(predictions, columns=['id', 'Predicted'])\n",
    "df_output.set_index('id')\n",
    "df_output.to_csv('ANN_attempt_2.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['id', 'Predicted']\n",
    "df_output = df_output[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.67061865, 0.61808383, 0.48727542, ..., 0.65179896, 0.621622  ,\n",
       "       0.66336346], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ndarray.flatten(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>592380</td>\n",
       "      <td>0.670619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>592381</td>\n",
       "      <td>0.618084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>592382</td>\n",
       "      <td>0.487275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>592383</td>\n",
       "      <td>0.512512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>592384</td>\n",
       "      <td>0.572973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  Predicted\n",
       "0  592380   0.670619\n",
       "1  592381   0.618084\n",
       "2  592382   0.487275\n",
       "3  592383   0.512512\n",
       "4  592384   0.572973"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ids = list(df_test['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[592380,\n",
       " 592381,\n",
       " 592382,\n",
       " 592383,\n",
       " 592384,\n",
       " 592385,\n",
       " 592386,\n",
       " 592387,\n",
       " 592388,\n",
       " 592389]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
